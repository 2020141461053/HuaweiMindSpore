{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7330844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.common.tensor import Tensor\n",
    "import mindspore.common.dtype as mstype\n",
    "\n",
    "def conv(in_channels, out_channels, kernel_size, stride=1, padding=0, pad_mode=\"valid\", has_bias=True):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                     has_bias=has_bias, pad_mode=pad_mode)\n",
    "\n",
    "def fc_with_initialize(input_channels, out_channels, has_bias=True):\n",
    "    return nn.Dense(input_channels, out_channels, has_bias=has_bias)\n",
    "\n",
    "class DataNormTranspose(nn.Cell):\n",
    "    \"\"\"Normalize an tensor image with mean and standard deviation.\n",
    "\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respectively.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respectively.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_name='imagenet'):\n",
    "        super(DataNormTranspose, self).__init__()\n",
    "        # Computed from random subset of ImageNet training images\n",
    "        if dataset_name == 'imagenet':\n",
    "            self.mean = Tensor(np.array([0.485 * 255, 0.456 * 255, 0.406 * 255]).reshape((1, 1, 1, 3)), mstype.float32)\n",
    "            self.std = Tensor(np.array([0.229 * 255, 0.224 * 255, 0.225 * 255]).reshape((1, 1, 1, 3)), mstype.float32)\n",
    "        else:\n",
    "            self.mean = Tensor(np.array([0.4914, 0.4822, 0.4465]).reshape((1, 1, 1, 3)), mstype.float32)\n",
    "            self.std = Tensor(np.array([0.2023, 0.1994, 0.2010]).reshape((1, 1, 1, 3)), mstype.float32)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = (x - self.mean) / self.std\n",
    "        x = F.transpose(x, (0, 3, 1, 2))\n",
    "        return x\n",
    "\n",
    "class AlexNet(nn.Cell):\n",
    "    \"\"\"\n",
    "    Alexnet\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, channel=3, phase='train', include_top=True, dataset_name='imagenet'):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.data_trans = DataNormTranspose(dataset_name=dataset_name)\n",
    "        self.conv1 = conv(channel, 64, 11, stride=4, pad_mode=\"same\", has_bias=True)\n",
    "        self.conv2 = conv(64, 128, 5, pad_mode=\"same\", has_bias=True)\n",
    "        self.conv3 = conv(128, 192, 3, pad_mode=\"same\", has_bias=True)\n",
    "        self.conv4 = conv(192, 256, 3, pad_mode=\"same\", has_bias=True)\n",
    "        self.conv5 = conv(256, 256, 3, pad_mode=\"same\", has_bias=True)\n",
    "        self.relu = P.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode='valid')\n",
    "        self.include_top = include_top\n",
    "        if self.include_top:\n",
    "            dropout_ratio = 0.65\n",
    "            if phase == 'test':\n",
    "                dropout_ratio = 1.0\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.fc1 = fc_with_initialize(6 * 6 * 256, 4096)\n",
    "            self.fc2 = fc_with_initialize(4096, 4096)\n",
    "            self.fc3 = fc_with_initialize(4096, num_classes)\n",
    "            self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"define network\"\"\"\n",
    "        x = self.data_trans(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        if not self.include_top:\n",
    "            return x\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import cpu_count\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.transforms as C\n",
    "import mindspore.dataset.vision as CV\n",
    "from mindspore.common import dtype as mstype\n",
    "from mindspore.communication.management import get_rank, get_group_size\n",
    "\n",
    "\n",
    "def create_dataset_cifar10(cfg, data_path, batch_size=32, status=\"train\", target=\"Ascend\",\n",
    "                           num_parallel_workers=8):\n",
    "    \"\"\"\n",
    "    create dataset for train or test\n",
    "    \"\"\"\n",
    "\n",
    "    ds.config.set_prefetch_size(64)\n",
    "    if target == \"Ascend\":\n",
    "        device_num, rank_id = _get_rank_info()\n",
    "\n",
    "    if target != \"Ascend\" or device_num == 1:\n",
    "        cifar_ds = ds.Cifar10Dataset(data_path, shuffle=True)\n",
    "    else:\n",
    "        cifar_ds = ds.Cifar10Dataset(data_path, num_parallel_workers=num_parallel_workers,\n",
    "                                     shuffle=True, num_shards=device_num, shard_id=rank_id)\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "    # cfg = alexnet_cifar10_cfg\n",
    "\n",
    "    resize_op = CV.Resize((cfg.image_height, cfg.image_width))\n",
    "    rescale_op = CV.Rescale(rescale, shift)\n",
    "    if status == \"train\":\n",
    "        random_crop_op = CV.RandomCrop([32, 32], [4, 4, 4, 4])\n",
    "        random_horizontal_op = CV.RandomHorizontalFlip()\n",
    "    typecast_op = C.TypeCast(mstype.int32)\n",
    "    cifar_ds = cifar_ds.map(input_columns=\"label\", operations=typecast_op,\n",
    "                            num_parallel_workers=1)\n",
    "    if status == \"train\":\n",
    "        compose_op = [random_crop_op, random_horizontal_op, resize_op, rescale_op]\n",
    "    else:\n",
    "        compose_op = [resize_op, rescale_op]\n",
    "    cifar_ds = cifar_ds.map(input_columns=\"image\", operations=compose_op, num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "    cifar_ds = cifar_ds.batch(batch_size, drop_remainder=True)\n",
    "    return cifar_ds\n",
    "\n",
    "\n",
    "def create_dataset_imagenet(cfg, dataset_path, batch_size=32, repeat_num=1, training=True,\n",
    "                            num_parallel_workers=16, shuffle=None, sampler=None, class_indexing=None):\n",
    "    \"\"\"\n",
    "    create a train or eval imagenet2012 dataset for resnet50\n",
    "\n",
    "    Args:\n",
    "        dataset_path(string): the path of dataset.\n",
    "        do_train(bool): whether dataset is used for train or eval.\n",
    "        repeat_num(int): the repeat times of dataset. Default: 1\n",
    "        batch_size(int): the batch size of dataset. Default: 32\n",
    "        target(str): the device target. Default: Ascend\n",
    "\n",
    "    Returns:\n",
    "        dataset\n",
    "    \"\"\"\n",
    "\n",
    "    device_num, rank_id = _get_rank_info()\n",
    "    # cfg = alexnet_imagenet_cfg\n",
    "\n",
    "    if device_num == 1:\n",
    "        num_parallel_workers = 96\n",
    "        if num_parallel_workers > cpu_count():\n",
    "            num_parallel_workers = cpu_count()\n",
    "    else:\n",
    "        ds.config.set_numa_enable(True)\n",
    "    data_set = ds.ImageFolderDataset(dataset_path, num_parallel_workers=4,\n",
    "                                     shuffle=shuffle, sampler=sampler, class_indexing=class_indexing,\n",
    "                                     num_shards=device_num, shard_id=rank_id)\n",
    "\n",
    "    assert cfg.image_height == cfg.image_width, \"imagenet_cfg.image_height not equal imagenet_cfg.image_width\"\n",
    "    image_size = cfg.image_height\n",
    "\n",
    "    # define map operations\n",
    "    transform_img = []\n",
    "    if training:\n",
    "        transform_img = [\n",
    "            CV.RandomCropDecodeResize(image_size, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
    "            CV.RandomHorizontalFlip(prob=0.5)\n",
    "        ]\n",
    "    else:\n",
    "        transform_img = [\n",
    "            CV.Decode(),\n",
    "            CV.Resize((256, 256)),\n",
    "            CV.CenterCrop(image_size)\n",
    "        ]\n",
    "\n",
    "    data_set = data_set.map(input_columns=\"image\", num_parallel_workers=num_parallel_workers,\n",
    "                            operations=transform_img)\n",
    "\n",
    "    data_set = data_set.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # apply dataset repeat operation\n",
    "    if repeat_num > 1:\n",
    "        data_set = data_set.repeat(repeat_num)\n",
    "\n",
    "    return data_set\n",
    "\n",
    "\n",
    "def _get_rank_info():\n",
    "    \"\"\"\n",
    "    get rank size and rank id\n",
    "    \"\"\"\n",
    "    rank_size = int(os.environ.get(\"RANK_SIZE\", 1))\n",
    "\n",
    "    if rank_size > 1:\n",
    "        rank_size = get_group_size()\n",
    "        rank_id = get_rank()\n",
    "    else:\n",
    "        rank_size = 1\n",
    "        rank_id = 0\n",
    "\n",
    "    return rank_size, rank_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ccc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "def get_lr_cifar10(current_step, lr_max, total_epochs, steps_per_epoch):\n",
    "    \"\"\"\n",
    "    generate learning rate array\n",
    "\n",
    "    Args:\n",
    "       current_step(int): current steps of the training\n",
    "       lr_max(float): max learning rate\n",
    "       total_epochs(int): total epoch of training\n",
    "       steps_per_epoch(int): steps of one epoch\n",
    "\n",
    "    Returns:\n",
    "       np.array, learning rate array\n",
    "    \"\"\"\n",
    "    lr_each_step = []\n",
    "    total_steps = steps_per_epoch * total_epochs\n",
    "    decay_epoch_index = [0.8 * total_steps]\n",
    "    for i in range(total_steps):\n",
    "        if i < decay_epoch_index[0]:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = lr_max * 0.1\n",
    "        lr_each_step.append(lr)\n",
    "    lr_each_step = np.array(lr_each_step).astype(np.float32)\n",
    "    learning_rate = lr_each_step[current_step:]\n",
    "\n",
    "    return learning_rate\n",
    "\n",
    "def get_lr_imagenet(lr, epoch_size, steps_per_epoch):\n",
    "    \"\"\"generate learning rate array\"\"\"\n",
    "    lr = warmup_cosine_annealing_lr(lr, epoch_size, steps_per_epoch)\n",
    "    return lr\n",
    "\n",
    "def linear_warmup_lr(current_step, warmup_steps, base_lr, init_lr):\n",
    "    \"\"\"Linear learning rate\"\"\"\n",
    "    lr_inc = (float(base_lr) - float(init_lr)) / float(warmup_steps)\n",
    "    lr = float(init_lr) + lr_inc * current_step\n",
    "    return lr\n",
    "\n",
    "def warmup_cosine_annealing_lr(lr, max_epoch, steps_per_epoch, warmup_epochs=5, T_max=150, eta_min=0.0):\n",
    "    \"\"\" Cosine annealing learning rate\"\"\"\n",
    "    base_lr = lr\n",
    "    warmup_init_lr = 0\n",
    "    total_steps = int(max_epoch * steps_per_epoch)\n",
    "    warmup_steps = int(warmup_epochs * steps_per_epoch)\n",
    "\n",
    "    lr_each_step = []\n",
    "    for i in range(total_steps):\n",
    "        last_epoch = i // steps_per_epoch\n",
    "        if i < warmup_steps:\n",
    "            lr = linear_warmup_lr(i + 1, warmup_steps, base_lr, warmup_init_lr)\n",
    "        else:\n",
    "            lr = eta_min + (base_lr - eta_min) * (1. + math.cos(math.pi*last_epoch / T_max)) / 2\n",
    "        lr_each_step.append(lr)\n",
    "\n",
    "    return np.array(lr_each_step).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56110204",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"get parameters for Momentum optimizer\"\"\"\n",
    "def get_param_groups(network):\n",
    "    \"\"\"get parameters\"\"\"\n",
    "    decay_params = []\n",
    "    no_decay_params = []\n",
    "    for x in network.trainable_params():\n",
    "        parameter_name = x.name\n",
    "        if parameter_name.endswith('.bias'):\n",
    "            # all bias not using weight decay\n",
    "            no_decay_params.append(x)\n",
    "        elif parameter_name.endswith('.gamma'):\n",
    "            # bn weight bias not using weight decay, be carefully for now x not include BN\n",
    "            no_decay_params.append(x)\n",
    "        elif parameter_name.endswith('.beta'):\n",
    "            # bn weight bias not using weight decay, be carefully for now x not include BN\n",
    "            no_decay_params.append(x)\n",
    "        else:\n",
    "            decay_params.append(x)\n",
    "\n",
    "    return [{'params': no_decay_params, 'weight_decay': 0.0}, {'params': decay_params}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2b6019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import argparse\n",
    "from pprint import pprint, pformat\n",
    "import yaml\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration namespace. Convert dictionary to members.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg_dict):\n",
    "        for k, v in cfg_dict.items():\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                setattr(self, k, [Config(x) if isinstance(x, dict) else x for x in v])\n",
    "            else:\n",
    "                setattr(self, k, Config(v) if isinstance(v, dict) else v)\n",
    "\n",
    "    def __str__(self):\n",
    "        return pformat(self.__dict__)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "def parse_cli_to_yaml(parser, cfg, helper=None, choices=None, cfg_path=\"default_config.yaml\"):\n",
    "    \"\"\"\n",
    "    Parse command line arguments to the configuration according to the default yaml.\n",
    "\n",
    "    Args:\n",
    "        parser: Parent parser.\n",
    "        cfg: Base configuration.\n",
    "        helper: Helper description.\n",
    "        cfg_path: Path to the default yaml config.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"[REPLACE THIS at config.py]\",\n",
    "                                     parents=[parser])\n",
    "    helper = {} if helper is None else helper\n",
    "    choices = {} if choices is None else choices\n",
    "    for item in cfg:\n",
    "        if not isinstance(cfg[item], list) and not isinstance(cfg[item], dict):\n",
    "            help_description = helper[item] if item in helper else \"Please reference to {}\".format(cfg_path)\n",
    "            choice = choices[item] if item in choices else None\n",
    "            if isinstance(cfg[item], bool):\n",
    "                parser.add_argument(\"--\" + item, type=ast.literal_eval, default=cfg[item], choices=choice,\n",
    "                                    help=help_description)\n",
    "            else:\n",
    "                parser.add_argument(\"--\" + item, type=type(cfg[item]), default=cfg[item], choices=choice,\n",
    "                                    help=help_description)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def parse_yaml(yaml_path):\n",
    "    \"\"\"\n",
    "    Parse the yaml config file.\n",
    "\n",
    "    Args:\n",
    "        yaml_path: Path to the yaml config.\n",
    "    \"\"\"\n",
    "    with open(yaml_path, 'r') as fin:\n",
    "        try:\n",
    "            cfgs = yaml.load_all(fin.read(), Loader=yaml.FullLoader)\n",
    "            cfgs = [x for x in cfgs]\n",
    "            if len(cfgs) == 1:\n",
    "                cfg_helper = {}\n",
    "                cfg = cfgs[0]\n",
    "                cfg_choices = {}\n",
    "            elif len(cfgs) == 2:\n",
    "                cfg, cfg_helper = cfgs\n",
    "                cfg_choices = {}\n",
    "            elif len(cfgs) == 3:\n",
    "                cfg, cfg_helper, cfg_choices = cfgs\n",
    "            else:\n",
    "                raise ValueError(\"At most 3 docs (config, description for help, choices) are supported in config yaml\")\n",
    "            print(cfg_helper)\n",
    "        except:\n",
    "            raise ValueError(\"Failed to parse yaml\")\n",
    "    return cfg, cfg_helper, cfg_choices\n",
    "\n",
    "\n",
    "def merge(args, cfg):\n",
    "    \"\"\"\n",
    "    Merge the base config from yaml file and command line arguments.\n",
    "\n",
    "    Args:\n",
    "        args: Command line arguments.\n",
    "        cfg: Base configuration.\n",
    "    \"\"\"\n",
    "    args_var = vars(args)\n",
    "    for item in args_var:\n",
    "        cfg[item] = args_var[item]\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    \"\"\"\n",
    "    Get Config according to the yaml file and cli arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"default name\", add_help=False)\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    parser.add_argument(\"--config_path\", type=str, default=os.path.join(current_dir, \"../../default_config.yaml\"),\n",
    "                        help=\"Config file path\")\n",
    "    path_args, _ = parser.parse_known_args()\n",
    "    default, helper, choices = parse_yaml(path_args.config_path)\n",
    "    args = parse_cli_to_yaml(parser=parser, cfg=default, helper=helper, choices=choices, cfg_path=path_args.config_path)\n",
    "    final_config = merge(args, default)\n",
    "    pprint(final_config)\n",
    "    print(\"Please check the above information for the configurations\", flush=True)\n",
    "    return Config(final_config)\n",
    "\n",
    "config = get_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22078def",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Moxing adapter for ModelArts\"\"\"\n",
    "\n",
    "import os\n",
    "import functools\n",
    "from mindspore import context\n",
    "from mindspore.profiler import Profiler\n",
    "from .config import config\n",
    "\n",
    "_global_sync_count = 0\n",
    "\n",
    "def get_device_id():\n",
    "    device_id = os.getenv('DEVICE_ID', '0')\n",
    "    return int(device_id)\n",
    "\n",
    "\n",
    "def get_device_num():\n",
    "    device_num = os.getenv('RANK_SIZE', '1')\n",
    "    return int(device_num)\n",
    "\n",
    "\n",
    "def get_rank_id():\n",
    "    global_rank_id = os.getenv('RANK_ID', '0')\n",
    "    return int(global_rank_id)\n",
    "\n",
    "\n",
    "def get_job_id():\n",
    "    job_id = os.getenv('JOB_ID')\n",
    "    job_id = job_id if job_id != \"\" else \"default\"\n",
    "    return job_id\n",
    "\n",
    "def sync_data(from_path, to_path):\n",
    "    \"\"\"\n",
    "    Download data from remote obs to local directory if the first url is remote url and the second one is local path\n",
    "    Upload data from local directory to remote obs in contrast.\n",
    "    \"\"\"\n",
    "    import moxing as mox\n",
    "    import time\n",
    "    global _global_sync_count\n",
    "    sync_lock = \"/tmp/copy_sync.lock\" + str(_global_sync_count)\n",
    "    _global_sync_count += 1\n",
    "\n",
    "    # Each server contains 8 devices as most.\n",
    "    if get_device_id() % min(get_device_num(), 8) == 0 and not os.path.exists(sync_lock):\n",
    "        print(\"from path: \", from_path)\n",
    "        print(\"to path: \", to_path)\n",
    "        mox.file.copy_parallel(from_path, to_path)\n",
    "        print(\"===finish data synchronization===\")\n",
    "        try:\n",
    "            os.mknod(sync_lock)\n",
    "        except IOError:\n",
    "            pass\n",
    "        print(\"===save flag===\")\n",
    "\n",
    "    while True:\n",
    "        if os.path.exists(sync_lock):\n",
    "            break\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(\"Finish sync data from {} to {}.\".format(from_path, to_path))\n",
    "\n",
    "\n",
    "def moxing_wrapper(pre_process=None, post_process=None):\n",
    "    \"\"\"\n",
    "    Moxing wrapper to download dataset and upload outputs.\n",
    "    \"\"\"\n",
    "    def wrapper(run_func):\n",
    "        @functools.wraps(run_func)\n",
    "        def wrapped_func(*args, **kwargs):\n",
    "            # Download data from data_url\n",
    "            if config.enable_modelarts:\n",
    "                if config.data_url:\n",
    "                    sync_data(config.data_url, config.data_path)\n",
    "                    print(\"Dataset downloaded: \", os.listdir(config.data_path))\n",
    "                if config.checkpoint_url:\n",
    "                    sync_data(config.checkpoint_url, config.load_path)\n",
    "                    print(\"Preload downloaded: \", os.listdir(config.load_path))\n",
    "                if config.train_url:\n",
    "                    sync_data(config.train_url, config.output_path)\n",
    "                    print(\"Workspace downloaded: \", os.listdir(config.output_path))\n",
    "\n",
    "                context.set_context(save_graphs_path=os.path.join(config.output_path, str(get_rank_id())))\n",
    "                config.device_num = get_device_num()\n",
    "                config.device_id = get_device_id()\n",
    "                if not os.path.exists(config.output_path):\n",
    "                    os.makedirs(config.output_path)\n",
    "\n",
    "                if pre_process:\n",
    "                    pre_process()\n",
    "\n",
    "            if config.enable_profiling:\n",
    "                profiler = Profiler()\n",
    "\n",
    "            run_func(*args, **kwargs)\n",
    "\n",
    "            if config.enable_profiling:\n",
    "                profiler.analyse()\n",
    "\n",
    "            # Upload data to train_url\n",
    "            if config.enable_modelarts:\n",
    "                if post_process:\n",
    "                    post_process()\n",
    "\n",
    "                if config.train_url:\n",
    "                    print(\"Start to copy output directory\")\n",
    "                    sync_data(config.output_path, config.train_url)\n",
    "        return wrapped_func\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d86504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.enable_modelarts:\n",
    "    from .moxing_adapter import get_device_id, get_device_num, get_rank_id, get_job_id\n",
    "else:\n",
    "    from .local_adapter import get_device_id, get_device_num, get_rank_id, get_job_id\n",
    "\n",
    "__all__ = [\n",
    "    \"get_device_id\", \"get_device_num\", \"get_rank_id\", \"get_job_id\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a60987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mindspore.nn as nn\n",
    "from mindspore.communication.management import init, get_rank\n",
    "from mindspore import dataset as de\n",
    "from mindspore import context\n",
    "from mindspore import Tensor\n",
    "from mindspore.train import Model\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore.common import set_seed\n",
    "\n",
    "set_seed(100)\n",
    "de.config.set_seed(100)\n",
    "\n",
    "def modelarts_pre_process():\n",
    "    pass\n",
    "    # config.ckpt_path = os.path.join(config.output_path, str(get_rank_id()), config.checkpoint_path)\n",
    "\n",
    "@moxing_wrapper(pre_process=modelarts_pre_process)\n",
    "def train_alexnet():\n",
    "    print('device id:', get_device_id())\n",
    "    print('device num:', get_device_num())\n",
    "    print('rank id:', get_rank_id())\n",
    "    print('job id:', get_job_id())\n",
    "\n",
    "    device_target = config.device_target\n",
    "    context.set_context(mode=context.GRAPH_MODE, device_target=config.device_target)\n",
    "    context.set_context(save_graphs=False)\n",
    "    if device_target == \"GPU\":\n",
    "        context.set_context(enable_graph_kernel=True)\n",
    "        context.set_context(graph_kernel_flags=\"--enable_cluster_ops=MatMul\")\n",
    "\n",
    "    device_num = get_device_num()\n",
    "\n",
    "    if device_num > 1:\n",
    "        context.reset_auto_parallel_context()\n",
    "        context.set_auto_parallel_context(device_num=device_num, \\\n",
    "            parallel_mode=ParallelMode.DATA_PARALLEL, gradients_mean=True)\n",
    "        if device_target == \"Ascend\":\n",
    "            context.set_context(device_id=get_device_id())\n",
    "            init()\n",
    "        elif device_target == \"GPU\":\n",
    "            init()\n",
    "    else:\n",
    "        context.set_context(device_id=get_device_id())\n",
    "\n",
    "    if config.dataset_name == \"cifar10\":\n",
    "        ds_train = create_dataset_cifar10(config, config.data_path, config.batch_size, target=config.device_target)\n",
    "    elif config.dataset_name == \"imagenet\":\n",
    "        # Imagenet dataset normalize and transpose will work on device\n",
    "        ds_train = create_dataset_imagenet(config, config.data_path, config.batch_size)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset.\")\n",
    "\n",
    "    if ds_train.get_dataset_size() == 0:\n",
    "        raise ValueError(\"Please check dataset size > 0 and batch_size <= dataset size\")\n",
    "\n",
    "    network = AlexNet(config.num_classes, phase='train', dataset_name=config.dataset_name)\n",
    "    loss_scale_manager = None\n",
    "    metrics = None\n",
    "    step_per_epoch = ds_train.get_dataset_size() if config.sink_size == -1 else config.sink_size\n",
    "    if config.dataset_name == 'cifar10':\n",
    "        loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
    "        lr = Tensor(get_lr_cifar10(0, config.learning_rate, config.epoch_size, step_per_epoch))\n",
    "        opt = nn.Momentum(network.trainable_params(), lr, config.momentum)\n",
    "        metrics = {\"Accuracy\": Accuracy()}\n",
    "\n",
    "    elif config.dataset_name == 'imagenet':\n",
    "        loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
    "        lr = Tensor(get_lr_imagenet(config.learning_rate, config.epoch_size, step_per_epoch))\n",
    "        opt = nn.Momentum(params=get_param_groups(network),\n",
    "                          learning_rate=lr,\n",
    "                          momentum=config.momentum,\n",
    "                          weight_decay=config.weight_decay,\n",
    "                          loss_scale=config.loss_scale)\n",
    "\n",
    "        from mindspore.train.loss_scale_manager import DynamicLossScaleManager, FixedLossScaleManager\n",
    "        if config.is_dynamic_loss_scale == 1:\n",
    "            loss_scale_manager = DynamicLossScaleManager(init_loss_scale=65536, scale_factor=2, scale_window=2000)\n",
    "        else:\n",
    "            loss_scale_manager = FixedLossScaleManager(config.loss_scale, drop_overflow_update=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset.\")\n",
    "\n",
    "    if device_target == \"Ascend\":\n",
    "        model = Model(network, loss_fn=loss, optimizer=opt, metrics=metrics, amp_level=\"O2\", keep_batchnorm_fp32=False,\n",
    "                      loss_scale_manager=loss_scale_manager)\n",
    "    elif device_target == \"GPU\":\n",
    "        model = Model(network, loss_fn=loss, optimizer=opt, metrics=metrics, amp_level=\"O2\",\n",
    "                      loss_scale_manager=loss_scale_manager)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported platform.\")\n",
    "\n",
    "    if device_num > 1:\n",
    "        ckpt_save_dir = os.path.join(config.ckpt_path + \"_\" + str(get_rank()))\n",
    "    else:\n",
    "        ckpt_save_dir = config.ckpt_path\n",
    "\n",
    "    time_cb = TimeMonitor(data_size=step_per_epoch)\n",
    "    config_ck = CheckpointConfig(save_checkpoint_steps=config.save_checkpoint_steps,\n",
    "                                 keep_checkpoint_max=config.keep_checkpoint_max)\n",
    "    ckpoint_cb = ModelCheckpoint(prefix=\"checkpoint_alexnet\", directory=ckpt_save_dir, config=config_ck)\n",
    "\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    model.train(config.epoch_size, ds_train, callbacks=[time_cb, ckpoint_cb, LossMonitor()],\n",
    "                dataset_sink_mode=config.dataset_sink_mode, sink_size=config.sink_size)\n",
    "\n",
    "\n",
    "train_alexnet()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('vatt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "43de742aca4a4bfb4ea878bb54627a9730249d6d36116ae6ea052e0f91f8870b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
